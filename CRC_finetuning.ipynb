{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c31c481",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from enformer_pytorch import from_pretrained, Enformer\n",
    "from enformer_pytorch.finetune import HeadAdapterWrapper\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchmetrics\n",
    "from torchmetrics.regression import PearsonCorrCoef"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d75aa2",
   "metadata": {},
   "source": [
    "# Enformer Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249e8472",
   "metadata": {},
   "source": [
    "In this notebook, we are going to attempt to fine-tune the Enformer model and optimize the parameters needed. Here is a list of parameters we are going to test: \n",
    "\n",
    "**1)** Use just the TCGA data for fine-tuning (already confirmed that it works better. See PCA).\\\n",
    "**2)** Ensure data loader works properly and we get the sequences we want. \\\n",
    "**3)** Normalize gene counts and accordingly remove the soft max layer at the end of the model. Also means using MSE as loss function. \\\n",
    "**4)** Take the left-most tensor instead of averaging the middle two tensors together. Better metric. \n",
    "\n",
    "**NOTE**: this notebook can be adjusted based on different parameters - ideas include predicting raw vs. normalized counts, max-pooling/optimizing which tensor to pick, learning rate change, downsampling - feel free to edit this notebook accordingly. \n",
    "\n",
    "**DOWNSAMPLE TRAINING DATA NOW**: we will use only 1500 genes/sequences in order to save time for our training. The goal is just to see if we CAN overfit on new training data. We can comment out the downsampling step below to fine-tune on all of the genes.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8d4561",
   "metadata": {},
   "source": [
    "### Filter for just TCGA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33f93d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "table {float:left}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b080ce45",
   "metadata": {},
   "source": [
    "| Cohort | Donors | Samples |\n",
    "|:------:|:---:|:---:|\n",
    "| TCGA-COAD | 459 | 522 |\n",
    "| TCGA-READ | 168 | 173 |\n",
    "| HCMI-CMDC | 66 | 138 |\n",
    "| **TOTAL** | **693** | **833** | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "558b9988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load gene counts and metadata in\n",
    "gene_counts = pd.read_csv('/pollard/home/aravi1/CRC_TCGA_HCMI_data/crc_gene_counts.tsv', sep='\\t')\n",
    "metadata = pd.read_csv('/pollard/home/aravi1/CRC_TCGA_HCMI_data/crc_metadata.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa45c51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename count columns to each biosample ID \n",
    "biosample_id = metadata['File.Name'].str.split('.').str[0]\n",
    "gene_counts.columns.values[3:] = biosample_id \n",
    "metadata['id'] = biosample_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62824680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gene_id</th>\n",
       "      <th>gene_name</th>\n",
       "      <th>gene_type</th>\n",
       "      <th>90c9f8cd-4c8c-4f07-af2f-e17db69bd561</th>\n",
       "      <th>0f35c851-1cb8-4f75-a661-eae9111b7362</th>\n",
       "      <th>ff11a9e3-d32b-431b-9ebb-c5a3d9eb0e4f</th>\n",
       "      <th>8ee55a63-4e87-4c00-8012-4c87efdcb7ed</th>\n",
       "      <th>5c3c4b79-0682-4f19-96aa-071316a354d4</th>\n",
       "      <th>05167d53-0b47-4131-bfa4-450b236b9fd5</th>\n",
       "      <th>a49e0bfd-c6f5-4fc5-9eb5-0eeab117124f</th>\n",
       "      <th>...</th>\n",
       "      <th>e356598b-7611-492a-98ed-ef2ec1b77b7a</th>\n",
       "      <th>a3380ca6-6c65-4543-9bdf-957b26c1daaf</th>\n",
       "      <th>c5f2e898-f42a-44a2-9b3f-8af491c99857</th>\n",
       "      <th>47661ed9-5c0e-442d-b072-3da8b14fab02</th>\n",
       "      <th>9ed0c331-aa64-4928-83d1-7fb6be9b0d24</th>\n",
       "      <th>5472fa65-a8e2-4593-abd8-e241d1bdec84</th>\n",
       "      <th>c98732eb-48c4-4554-bdf6-0e9a0a9273ec</th>\n",
       "      <th>9da01737-be8f-4af5-9f8f-47bb892b6339</th>\n",
       "      <th>dfb9c45f-cb52-4e36-b663-1a9e8c7c0b47</th>\n",
       "      <th>ad30e5e1-182b-4758-8e70-53e0bcf78072</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENSG00000000003.15</td>\n",
       "      <td>TSPAN6</td>\n",
       "      <td>protein_coding</td>\n",
       "      <td>102.6828</td>\n",
       "      <td>180.1091</td>\n",
       "      <td>196.9790</td>\n",
       "      <td>178.5252</td>\n",
       "      <td>200.7512</td>\n",
       "      <td>36.3378</td>\n",
       "      <td>103.4066</td>\n",
       "      <td>...</td>\n",
       "      <td>129.5642</td>\n",
       "      <td>167.1648</td>\n",
       "      <td>138.2400</td>\n",
       "      <td>122.9009</td>\n",
       "      <td>99.9213</td>\n",
       "      <td>329.6668</td>\n",
       "      <td>88.3904</td>\n",
       "      <td>84.0686</td>\n",
       "      <td>168.7268</td>\n",
       "      <td>187.4724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENSG00000000005.6</td>\n",
       "      <td>TNMD</td>\n",
       "      <td>protein_coding</td>\n",
       "      <td>0.5909</td>\n",
       "      <td>4.2922</td>\n",
       "      <td>7.0440</td>\n",
       "      <td>2.0690</td>\n",
       "      <td>1.9239</td>\n",
       "      <td>1.4758</td>\n",
       "      <td>0.2529</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5335</td>\n",
       "      <td>0.2034</td>\n",
       "      <td>0.3460</td>\n",
       "      <td>15.0110</td>\n",
       "      <td>0.5999</td>\n",
       "      <td>4.1491</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>4.6429</td>\n",
       "      <td>2.1025</td>\n",
       "      <td>5.4324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENSG00000000419.13</td>\n",
       "      <td>DPM1</td>\n",
       "      <td>protein_coding</td>\n",
       "      <td>189.5382</td>\n",
       "      <td>237.6030</td>\n",
       "      <td>259.5645</td>\n",
       "      <td>190.5531</td>\n",
       "      <td>156.6911</td>\n",
       "      <td>16.1225</td>\n",
       "      <td>134.7653</td>\n",
       "      <td>...</td>\n",
       "      <td>209.5608</td>\n",
       "      <td>175.5828</td>\n",
       "      <td>287.7883</td>\n",
       "      <td>74.4915</td>\n",
       "      <td>253.1841</td>\n",
       "      <td>269.6910</td>\n",
       "      <td>127.4410</td>\n",
       "      <td>106.8103</td>\n",
       "      <td>207.7205</td>\n",
       "      <td>234.1476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENSG00000000457.14</td>\n",
       "      <td>SCYL3</td>\n",
       "      <td>protein_coding</td>\n",
       "      <td>15.8945</td>\n",
       "      <td>6.3773</td>\n",
       "      <td>6.4526</td>\n",
       "      <td>9.2412</td>\n",
       "      <td>5.0334</td>\n",
       "      <td>2.0466</td>\n",
       "      <td>7.9265</td>\n",
       "      <td>...</td>\n",
       "      <td>14.2390</td>\n",
       "      <td>8.1337</td>\n",
       "      <td>13.0432</td>\n",
       "      <td>2.5129</td>\n",
       "      <td>9.1820</td>\n",
       "      <td>5.2358</td>\n",
       "      <td>13.4162</td>\n",
       "      <td>7.1353</td>\n",
       "      <td>11.1024</td>\n",
       "      <td>9.5908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENSG00000000460.17</td>\n",
       "      <td>C1orf112</td>\n",
       "      <td>protein_coding</td>\n",
       "      <td>14.0779</td>\n",
       "      <td>5.9787</td>\n",
       "      <td>6.6893</td>\n",
       "      <td>5.0715</td>\n",
       "      <td>4.6298</td>\n",
       "      <td>2.9191</td>\n",
       "      <td>10.1571</td>\n",
       "      <td>...</td>\n",
       "      <td>14.2176</td>\n",
       "      <td>6.0589</td>\n",
       "      <td>7.1255</td>\n",
       "      <td>1.9394</td>\n",
       "      <td>8.0648</td>\n",
       "      <td>6.3916</td>\n",
       "      <td>9.4148</td>\n",
       "      <td>2.7023</td>\n",
       "      <td>14.1215</td>\n",
       "      <td>5.8139</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 836 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              gene_id gene_name       gene_type  \\\n",
       "0  ENSG00000000003.15    TSPAN6  protein_coding   \n",
       "1   ENSG00000000005.6      TNMD  protein_coding   \n",
       "2  ENSG00000000419.13      DPM1  protein_coding   \n",
       "3  ENSG00000000457.14     SCYL3  protein_coding   \n",
       "4  ENSG00000000460.17  C1orf112  protein_coding   \n",
       "\n",
       "   90c9f8cd-4c8c-4f07-af2f-e17db69bd561  0f35c851-1cb8-4f75-a661-eae9111b7362  \\\n",
       "0                              102.6828                              180.1091   \n",
       "1                                0.5909                                4.2922   \n",
       "2                              189.5382                              237.6030   \n",
       "3                               15.8945                                6.3773   \n",
       "4                               14.0779                                5.9787   \n",
       "\n",
       "   ff11a9e3-d32b-431b-9ebb-c5a3d9eb0e4f  8ee55a63-4e87-4c00-8012-4c87efdcb7ed  \\\n",
       "0                              196.9790                              178.5252   \n",
       "1                                7.0440                                2.0690   \n",
       "2                              259.5645                              190.5531   \n",
       "3                                6.4526                                9.2412   \n",
       "4                                6.6893                                5.0715   \n",
       "\n",
       "   5c3c4b79-0682-4f19-96aa-071316a354d4  05167d53-0b47-4131-bfa4-450b236b9fd5  \\\n",
       "0                              200.7512                               36.3378   \n",
       "1                                1.9239                                1.4758   \n",
       "2                              156.6911                               16.1225   \n",
       "3                                5.0334                                2.0466   \n",
       "4                                4.6298                                2.9191   \n",
       "\n",
       "   a49e0bfd-c6f5-4fc5-9eb5-0eeab117124f  ...  \\\n",
       "0                              103.4066  ...   \n",
       "1                                0.2529  ...   \n",
       "2                              134.7653  ...   \n",
       "3                                7.9265  ...   \n",
       "4                               10.1571  ...   \n",
       "\n",
       "   e356598b-7611-492a-98ed-ef2ec1b77b7a  a3380ca6-6c65-4543-9bdf-957b26c1daaf  \\\n",
       "0                              129.5642                              167.1648   \n",
       "1                                1.5335                                0.2034   \n",
       "2                              209.5608                              175.5828   \n",
       "3                               14.2390                                8.1337   \n",
       "4                               14.2176                                6.0589   \n",
       "\n",
       "   c5f2e898-f42a-44a2-9b3f-8af491c99857  47661ed9-5c0e-442d-b072-3da8b14fab02  \\\n",
       "0                              138.2400                              122.9009   \n",
       "1                                0.3460                               15.0110   \n",
       "2                              287.7883                               74.4915   \n",
       "3                               13.0432                                2.5129   \n",
       "4                                7.1255                                1.9394   \n",
       "\n",
       "   9ed0c331-aa64-4928-83d1-7fb6be9b0d24  5472fa65-a8e2-4593-abd8-e241d1bdec84  \\\n",
       "0                               99.9213                              329.6668   \n",
       "1                                0.5999                                4.1491   \n",
       "2                              253.1841                              269.6910   \n",
       "3                                9.1820                                5.2358   \n",
       "4                                8.0648                                6.3916   \n",
       "\n",
       "   c98732eb-48c4-4554-bdf6-0e9a0a9273ec  9da01737-be8f-4af5-9f8f-47bb892b6339  \\\n",
       "0                               88.3904                               84.0686   \n",
       "1                                0.0000                                4.6429   \n",
       "2                              127.4410                              106.8103   \n",
       "3                               13.4162                                7.1353   \n",
       "4                                9.4148                                2.7023   \n",
       "\n",
       "   dfb9c45f-cb52-4e36-b663-1a9e8c7c0b47  ad30e5e1-182b-4758-8e70-53e0bcf78072  \n",
       "0                              168.7268                              187.4724  \n",
       "1                                2.1025                                5.4324  \n",
       "2                              207.7205                              234.1476  \n",
       "3                               11.1024                                9.5908  \n",
       "4                               14.1215                                5.8139  \n",
       "\n",
       "[5 rows x 836 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gene_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25ff8128",
   "metadata": {},
   "outputs": [],
   "source": [
    "tcga_samples = pd.concat([pd.Series(['gene_id', 'gene_name', 'gene_type']), metadata.id[metadata['Project'].str.contains('TCGA')]])\n",
    "gene_counts = gene_counts.loc[:, gene_counts.columns.isin(tcga_samples)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88937d13",
   "metadata": {},
   "source": [
    "Now, our gene count matrix only has TCGA samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8be6e8",
   "metadata": {},
   "source": [
    "### Normalization/median calculation + Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbf18fe",
   "metadata": {},
   "source": [
    "First, let's filter our count matrix to only the genes that were originally used to train Enformer. This way, we can maintain the same training/test splits that the original model used as to prevent bias.\n",
    "\n",
    "At this step, we can take two paths: \n",
    "- **Normalization:** for Enformer to predict normalized counts and to log-transform/normalize our target data, set this to True. If  normalization is False, Enformer resorts to default - predicting raw counts. When normalization=False, target data=raw counts. \n",
    "- **Filter Zero Genes:** to train Enformer on genes with non-zero TPM counts, set to True \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3819bca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change normalization parameter to true if you want Enformer to predict normalized TPM counts \n",
    "# rather than raw TPM counts \n",
    "\n",
    "# Set filter_zero_genes to True if we want to train model with only genes that have non-zero TPM expression\n",
    "normalization = False\n",
    "filter_zero_genes = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4ab4edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59427\n",
      "(59427, 698)\n"
     ]
    }
   ],
   "source": [
    "filtered_counts = gene_counts[gene_counts['gene_name'].duplicated() == False]\n",
    "\n",
    "\n",
    "# NORMALIZATION HERE - normalization means taking ln(TPM) - adding pseudocount \n",
    "if (normalization == True): \n",
    "    filtered_counts.iloc[:, 3:] = np.log(filtered_counts.iloc[:, 3:] + 0.00001)\n",
    "\n",
    "print(filtered_counts['gene_name'].nunique())\n",
    "print(filtered_counts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7328246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59427, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1724279/3305481067.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_counts['median'] = filtered_counts.iloc[:,3:].median(axis=1)\n"
     ]
    }
   ],
   "source": [
    "# Calculate median across samples here \n",
    "filtered_counts['median'] = filtered_counts.iloc[:,3:].median(axis=1)\n",
    "filtered_counts = filtered_counts[['gene_id', 'gene_name', 'gene_type', 'median']]\n",
    "\n",
    "if (filter_zero_genes == True):\n",
    "    filtered_counts = filtered_counts[filtered_counts['median'] < -7.5]\n",
    "    \n",
    "print(filtered_counts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3041d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17539, 4)\n"
     ]
    }
   ],
   "source": [
    "### INPUTS\n",
    "# training sequences\n",
    "# validation sequences\n",
    "# test sequences \n",
    "\n",
    "TSS_centered_genes = pd.read_csv('Enformer_genomic_regions_TSSCenteredGenes_FixedOverlapRemoval.csv')\n",
    "print(filtered_counts[filtered_counts['gene_name'].isin(TSS_centered_genes['gene_name'])].shape)\n",
    "filtered_counts = filtered_counts[filtered_counts['gene_name'].isin(TSS_centered_genes['gene_name'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4514248f",
   "metadata": {},
   "source": [
    "### Downsampling/Training-Test-Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea4ec7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "TSS_centered_genes = TSS_centered_genes[TSS_centered_genes['gene_name'].isin(filtered_counts['gene_name'])]\n",
    "\n",
    "# obtain training, validation and test genes here\n",
    "training_genes, test_genes, validation_genes = TSS_centered_genes['gene_name'][TSS_centered_genes['set'] == \"train\"], TSS_centered_genes['gene_name'][TSS_centered_genes['set'] == \"test\"], TSS_centered_genes['gene_name'][TSS_centered_genes['set'] == \"valid\"]\n",
    "\n",
    "# downsample training genes here - comment this out to train on all of the genes \n",
    "training_genes = training_genes.sample(n=1500, random_state=42)\n",
    "\n",
    "### INPUTS\n",
    "# training sequences\n",
    "# validation sequences\n",
    "# test sequences \n",
    "\n",
    "### shorten sequences from 196,608 bp to 49,152 as interval length. \n",
    "TSS_centered_genes['starts'] = (TSS_centered_genes['starts'] + ((196608 / 8) * 3)).astype(int)\n",
    "TSS_centered_genes['ends'] = (TSS_centered_genes['ends'] - ((196608 / 8) * 3)).astype(int)\n",
    "assert (TSS_centered_genes['ends'] - TSS_centered_genes['starts'] == 49152).all()\n",
    "assert (TSS_centered_genes['ends'] - TSS_centered_genes['gene_start'].astype(int) == 24576).all()\n",
    "\n",
    "training_sequences = TSS_centered_genes[TSS_centered_genes['gene_name'].isin(training_genes)]\n",
    "validation_sequences = TSS_centered_genes[TSS_centered_genes['gene_name'].isin(validation_genes)]\n",
    "test_sequences = TSS_centered_genes[TSS_centered_genes['gene_name'].isin(test_genes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "024ed058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1800"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94448ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### OUTPUTS\n",
    "# training tracks \n",
    "# validation tracks \n",
    "# testing tracks \n",
    "\n",
    "training_tracks = filtered_counts[filtered_counts['gene_name'].isin(training_genes)]\n",
    "validation_tracks = filtered_counts[filtered_counts['gene_name'].isin(validation_genes)]\n",
    "test_tracks = filtered_counts[filtered_counts['gene_name'].isin(test_genes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a34b81d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write input sequences to bed files for GenomeIntervalDataset to load to fine-tune\n",
    "\n",
    "training_sequences = pd.merge(training_sequences, training_tracks, on=\"gene_name\", how=\"inner\")\n",
    "validation_sequences = pd.merge(validation_sequences, validation_tracks, on=\"gene_name\", how=\"inner\")\n",
    "test_sequences = pd.merge(test_sequences, test_tracks, on=\"gene_name\", how=\"inner\")\n",
    "\n",
    "training_sequences.to_csv(\"training_sequences.bed\", sep='\\t', header=None, index=None)\n",
    "validation_sequences.to_csv(\"validation_sequences.bed\", sep='\\t', header=None, index=None)\n",
    "test_sequences.to_csv(\"test_sequences.bed\", sep='\\t', header=None, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffb97ee",
   "metadata": {},
   "source": [
    "### Data Loader/Batch Training-Test Split Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347d8b52",
   "metadata": {},
   "source": [
    "Next, let's ensure that our data loader works properly and we are getting the sequences that we want. Our sequence length that we will use is 49,152 bp (196,608 / 4). This allows us to effectively batch our sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3a3a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DEFINE TARGET VARIABLES \n",
    "target_length = 384\n",
    "TSS_tensor_pos1, TSS_tensor_pos2 = (target_length / 2) - 1, (target_length / 2)\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61dabd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "np.random.seed(150)\n",
    "\n",
    "import torch\n",
    "from enformer_pytorch import from_pretrained\n",
    "from enformer_pytorch.finetune import HeadAdapterWrapper\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import torch\n",
    "import polars as pl\n",
    "from enformer_pytorch import Enformer, GenomeIntervalDataset\n",
    "\n",
    "filter_train = lambda df: df.filter(pl.col('column_4') == 'train')\n",
    "filter_test = lambda df: df.filter(pl.col('column_4') == 'test')\n",
    "filter_valid = lambda df: df.filter(pl.col('column_4') == 'valid')\n",
    "\n",
    "# shift_augs by -64 to 64 try again \n",
    "training_ds = GenomeIntervalDataset(\n",
    "    bed_file = 'training_sequences.bed',                       # bed file - columns 0, 1, 2 must be <chromosome>, <start position>, <end position>\n",
    "    fasta_file = 'hg38.fa',                        # path to fasta file\n",
    "    filter_df_fn = filter_train,                        # filter dataframe function\n",
    "    return_seq_indices = True,                          # return nucleotide indices (ACGTN) or one hot encodings\n",
    "    # shift_augs = (-64, 64),                               # random shift augmentations from -2 to +2 basepairs\n",
    "    context_length = 49_152,\n",
    "    # this can be longer than the interval designated in the .bed file,\n",
    "    # in which case it will take care of lengthening the interval on either sides\n",
    "    # as well as proper padding if at the end of the chromosomes\n",
    ")\n",
    "\n",
    "test_ds = GenomeIntervalDataset(\n",
    "    bed_file = 'test_sequences.bed',                       \n",
    "    fasta_file = 'hg38.fa',\n",
    "    filter_df_fn = filter_test,\n",
    "    return_seq_indices = True,                         \n",
    "    context_length = 49_152,\n",
    "\n",
    ")\n",
    "\n",
    "validation_ds = GenomeIntervalDataset(\n",
    "    bed_file = 'validation_sequences.bed',                       \n",
    "    fasta_file = 'hg38.fa',\n",
    "    filter_df_fn = filter_valid,\n",
    "    return_seq_indices = True,                       \n",
    "    context_length = 49_152,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4afa8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dataset class to easily join input sequences and target TPM counts together. \n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, sequences, targets):\n",
    "        self.sequences = sequences\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = ((self.sequences[idx]) if len(self.sequences[idx]) > 0 else torch.zeros((1,)).clone().detach()).cuda() # Handling empty lists\n",
    "        target = torch.tensor(self.targets[idx]).cuda()\n",
    "        return sequence, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fdc6f6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = MyDataset(training_ds, training_sequences['median'])\n",
    "validation = MyDataset(validation_ds, validation_sequences['median'])\n",
    "test = MyDataset(test_ds, test_sequences['median'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da968e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader instances\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(150)\n",
    "\n",
    "train_loader = DataLoader(training, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(validation, batch_size=batch_size)\n",
    "test_loader = DataLoader(test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47902c91",
   "metadata": {},
   "source": [
    "## Model Architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e7c8212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Enformer revised/fine-tuned model here. \n",
    "from torch.nn import Sequential \n",
    "\n",
    "# 1,536 * 2 - pointwise convolutional\n",
    "# compute target length based on tensor shape \n",
    "\n",
    "class EnformerFineTuning(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        enformer,\n",
    "        num_tracks,\n",
    "        post_transformer_embed = False, # whether to take the embeddings from right after the transformer, instead of after the final pointwise convolutional - this would add another layernorm\n",
    "        ):\n",
    "        super().__init__()\n",
    "        assert isinstance(enformer, Enformer)\n",
    "        enformer_hidden_dim = enformer.dim * (2 if not post_transformer_embed else 1)\n",
    "        \n",
    "        self.enformer = enformer.cuda()\n",
    "        # predicting normalized counts - no softplus activation layer \n",
    "        if (normalization == True):\n",
    "            self.to_gene_counts = Sequential(\n",
    "                nn.Linear(enformer_hidden_dim, num_tracks, bias=True).cuda(),\n",
    "            )\n",
    "        # predicting raw counts - add softplus activation layer \n",
    "        else:\n",
    "            self.to_gene_counts = Sequential(\n",
    "                nn.Linear(enformer_hidden_dim, num_tracks, bias=True).cuda(),\n",
    "                nn.Softplus(beta=1, threshold=20).cuda()\n",
    "            )\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        seq,\n",
    "        target = None,\n",
    "        freeze_enformer = False,\n",
    "        finetune_enformer_ln_only = False,\n",
    "        finetune_last_n_layers_only = None,\n",
    "    ):\n",
    "        enformer_kwargs = dict()\n",
    "\n",
    "        # enformer_kwargs = dict(target_length = 1)\n",
    "        \n",
    "        # returning only the embeddings here \n",
    "        embeddings = self.enformer(seq, return_only_embeddings=True)\n",
    "        # print(embeddings.size())\n",
    "        \n",
    "        # batch_dim, seq_dim, feature_dim = embeddings.size()\n",
    "        \n",
    "        # compute center tensor by floor dividing - convention is just to take the left \n",
    "        \n",
    "        # take center positions of embedding - we take left tensor in the middle \n",
    "        TSS_tensor1 = embeddings[:, int(TSS_tensor_pos1)]\n",
    "        \n",
    "        # convert embedding to gene count value \n",
    "        preds = self.to_gene_counts(TSS_tensor1)\n",
    "\n",
    "        return preds\n",
    "    \n",
    "    def _log(self, t, eps = 1e-20):\n",
    "        return torch.log(t.clamp(min = eps))\n",
    "    \n",
    "    # LOSS FUNCTION for Enformer \n",
    "    def poisson_loss(self, pred, target):\n",
    "        return (pred - target * self._log(pred)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d52a66",
   "metadata": {},
   "source": [
    "## Fine-tuning: Training Step "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7250f3be",
   "metadata": {},
   "source": [
    "Finally, we will train and fine-tune our model in this step. After creating our data loader and preprocessing our target counts, we use a Poisson loss function for predicting raw counts + MSE loss for predicting log transformed counts. \n",
    "\n",
    "While the model converges in less than 3 iterations usually, we can control the iterations with num_iterations. We use an ADAM optimizer with a learning rate of 0.00005. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c2c4633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "Epoch 1, Loss: -62.16759, Accuracy: 0.43704\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "Epoch 1, Validation Loss: -64.62198406904345, Validation Accuracy: 0.61948\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "Epoch 2, Loss: -109.68409, Accuracy: 0.61692\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "Epoch 2, Validation Loss: -102.7616125268387, Validation Accuracy: 0.65311\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "Epoch 3, Loss: -122.40199, Accuracy: 0.65467\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "Epoch 3, Validation Loss: -103.62525866625177, Validation Accuracy: 0.66587\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "Epoch 4, Loss: -132.52928, Accuracy: 0.68618\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "Epoch 4, Validation Loss: -107.57043206828367, Validation Accuracy: 0.64918\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "Epoch 5, Loss: -142.23221, Accuracy: 0.68424\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "Epoch 5, Validation Loss: -112.09365465196903, Validation Accuracy: 0.62735\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained Enformer model\n",
    "# pytorch lightning - does everything for you \n",
    "enformer = from_pretrained('EleutherAI/enformer-official-rough', target_length=target_length, use_tf_gamma=False).cuda()\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "# Instantiate the HeadAdapterWrapper\n",
    "model = EnformerFineTuning(enformer=enformer, num_tracks=1, post_transformer_embed=False).cuda()\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "\n",
    "# Number of training iterations\n",
    "num_iterations = 5\n",
    "\n",
    "### Define optimizer here \n",
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00005)\n",
    "\n",
    "### loss function \n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# training variables \n",
    "training_targets=[]\n",
    "training_outputs=[]\n",
    "training_loss=[]\n",
    "\n",
    "# validation inputs\n",
    "validation_targets=[]\n",
    "validation_outputs=[]\n",
    "validation_loss=[]\n",
    "\n",
    "pearson = PearsonCorrCoef().to(\"cuda:0\")\n",
    "\n",
    "# gradients=[]\n",
    "# Training loop\n",
    "for i in range(num_iterations):\n",
    "    model.train()\n",
    "\n",
    "    j = 0\n",
    "    training_loss_epoch = []\n",
    "    outputs=[]\n",
    "    targets=[]\n",
    "    for seq, target in train_loader:\n",
    "        # Forward pass\n",
    "        \n",
    "        target = torch.reshape(target, [target.size()[0], 1])\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(seq)\n",
    "        \n",
    "        if (normalization == True):\n",
    "            loss = criterion(output.float(), target.float())\n",
    "        else: \n",
    "            loss = model.poisson_loss(output, target)\n",
    "        \n",
    "        training_loss_epoch.append(loss.item())\n",
    "\n",
    "        outputs.append(output)\n",
    "        targets.append(target)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        if (j % 100 == 0):\n",
    "            print(j)\n",
    "\n",
    "        j += 1\n",
    "        optimizer.step()\n",
    "        loss = loss.detach()\n",
    "    \n",
    "    training_outputs.append(outputs)\n",
    "    training_targets.append(targets)\n",
    "    training_loss.append(training_loss_epoch)\n",
    "    average_loss = np.mean(np.array(training_loss_epoch))\n",
    "        \n",
    "    # NORMALIZATION\n",
    "    if (normalization == True):\n",
    "        average_accuracy = pearson(torch.cat(outputs), torch.cat(targets))\n",
    "    else:\n",
    "        average_accuracy = pearson(torch.log(torch.cat(outputs) + 0.00001), torch.log(torch.cat(targets) + 0.00001))\n",
    "    \n",
    "    print(f\"Epoch {i+1}, Loss: {round(average_loss, 5)}, Accuracy: {round(average_accuracy.item(), 5)}\")\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    k = 0\n",
    "    val_loss_epoch = []\n",
    "    val_outputs=[]\n",
    "    val_targets=[]\n",
    "    with torch.no_grad():\n",
    "        for val_seq, val_target in val_loader:\n",
    "            \n",
    "            val_target = torch.reshape(val_target, [val_target.size()[0], 1])\n",
    "            \n",
    "            val_output = model(val_seq)\n",
    "            \n",
    "            if (normalization == True): \n",
    "                val_loss = criterion(val_output.float(), val_target.float())\n",
    "            else: \n",
    "                val_loss = model.poisson_loss(val_output, val_target)\n",
    "            \n",
    "            \n",
    "            val_loss_epoch.append(val_loss.item())\n",
    "            \n",
    "            val_outputs.append(val_output)\n",
    "            val_targets.append(val_target)\n",
    "            \n",
    "            if (k % 100 == 0):\n",
    "                print(k)\n",
    "                \n",
    "            k += 1\n",
    "\n",
    "        \n",
    "        validation_targets.append(val_targets)\n",
    "        validation_outputs.append(val_outputs)\n",
    "        validation_loss.append(val_loss_epoch)\n",
    "        \n",
    "        validation_loss_epoch = np.mean(np.array(val_loss_epoch))\n",
    "        \n",
    "        ### NORMALIZATION\n",
    "        if (normalization == True):\n",
    "            validation_accuracy = pearson(torch.cat(val_outputs), torch.cat(val_targets))\n",
    "        else: \n",
    "            validation_accuracy = pearson(torch.log(torch.cat(val_outputs) + 0.00001), torch.log(torch.cat(val_targets) + 0.00001))\n",
    "\n",
    "        print(f\"Epoch {i+1}, Validation Loss: {validation_loss_epoch}, Validation Accuracy: {round(validation_accuracy.item(), 5)}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac18d44b",
   "metadata": {},
   "source": [
    "## Fine-tuning: evaluate on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1b4a22a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Test Loss: 0.45113492012023926, Test Accuracy: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4146967/971399507.py:29: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  test_accuracy = pearsonr(torch.cat(test_outputs).cpu().detach().numpy().flatten(), torch.cat(test_targets).cpu().detach().numpy().flatten())\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a test_loader similar to train_loader and val_loader\n",
    "\n",
    "test_targets = []\n",
    "test_outputs = []\n",
    "test_loss = []\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "with torch.no_grad():\n",
    "    l = 0\n",
    "    for test_seq, test_target in test_loader:\n",
    "        test_target = test_target.reshape([test_target.size()[0], 1])\n",
    "        test_output = model(test_seq)\n",
    "        if (normalization == True):\n",
    "            test_loss_batch = criterion(test_output.float(), test_target.float())\n",
    "        else: \n",
    "            test_loss_batch = model.poisson_loss(test_output, test_target).item()\n",
    "\n",
    "        test_targets.append(test_target)\n",
    "        test_outputs.append(test_output)\n",
    "        test_loss.append(test_loss_batch)\n",
    "        \n",
    "        if (l % 100 == 0):\n",
    "            print(l)\n",
    "        \n",
    "        l += 1\n",
    "\n",
    "if (normalization == True):\n",
    "    test_accuracy = pearsonr(torch.cat(test_outputs).cpu().detach().numpy().flatten(), torch.cat(test_targets).cpu().detach().numpy().flatten())\n",
    "    test_loss_mean = np.mean(np.array([tensor.cpu().numpy() for tensor in test_loss]))\n",
    "else: \n",
    "    test_loss_mean = np.mean(np.array(test_loss))\n",
    "    test_accuracy = pearsonr(torch.log(torch.cat(test_outputs) + 0.00001).cpu().detach().numpy().flatten(), torch.log(torch.cat(test_targets) + 0.00001).cpu().detach().numpy().flatten())\n",
    "    \n",
    "print(f\"Test Loss: {test_loss_mean}, Test Accuracy: {round(test_accuracy[0], 5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d344208b",
   "metadata": {},
   "source": [
    "### Save fine-tuned model and all training/test/validation outputs for evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e45f01a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "file_path = '240506_enformer_raw/enformer_finetuned.pkl'\n",
    "\n",
    "with open(file_path, 'wb') as f:\n",
    "    pkl.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c9939d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training variables \n",
    "torch.save(training_targets, '240506_enformer_raw/training_targets.pt')\n",
    "torch.save(training_outputs, '240506_enformer_raw/training_outputs.pt')\n",
    "torch.save(training_loss, '240506_enformer_raw/training_loss.pt')\n",
    "\n",
    "# validation inputs\n",
    "torch.save(validation_targets, '240506_enformer_raw/validation_targets.pt')\n",
    "torch.save(validation_outputs, '240506_enformer_raw/validation_outputs.pt')\n",
    "torch.save(validation_loss, '240506_enformer_raw/validation_loss.pt')\n",
    "\n",
    "torch.save(test_targets, '240506_enformer_raw/test_targets.pt')\n",
    "torch.save(test_outputs, '240506_enformer_raw/test_outputs.pt')\n",
    "torch.save(test_loss, '240506_enformer_raw/test_loss.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
